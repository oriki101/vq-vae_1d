{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vq_vae import VQ_VAE\n",
    "from utils.emg_datasets import EMGDataset\n",
    "from utils.emg_datasets import EMGDatasetValid\n",
    "from utils.emg_datasets import EMGDatasetTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F \n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_open = open(\"../configurations/setting.json\", mode='r')\n",
    "json_load = json.load(json_open)\n",
    "json_open.close()\n",
    "num_hiddens = json_load['num_hiddens']\n",
    "num_residual_hiddens = json_load['num_residual_hiddens']\n",
    "num_residual_layers = json_load[\"num_residual_layers\"]\n",
    "embedding_dim = json_load[\"embedding_dim\"]\n",
    "num_embeddings = json_load[\"num_embeddings\"]\n",
    "commitment_cost = json_load[\"commitment_cost\"]\n",
    "decay = json_load[\"decay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 32\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "\n",
    "# embedding_dim = 1 #RJS最初\n",
    "embedding_dim = 128\n",
    "num_embeddings = 128\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_training_updates = 100\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EMGDataset(transform=transforms.ToTensor())\n",
    "valid_dataset = EMGDatasetValid(transform=transforms.ToTensor())\n",
    "test_dataset = EMGDatasetTest(transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQ_VAE(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "recon_error_s = 0\n",
    "res_perplexity = 0\n",
    "data_variance = 1\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "\n",
    "f_name = \"emg_vq_vae-512-8dim-2\"\n",
    "dt_now = datetime.datetime.now()\n",
    "now = \"{}-{}-{}\".format(dt_now.year, dt_now.month, dt_now.day)\n",
    "name_dir = \"pth/{}\".format(now)\n",
    "name = \"pth/{}/{}.pth\".format(now, f_name)\n",
    "if not os.path.isdir(name_dir):\n",
    "    os.makedirs(name_dir)\n",
    "log_dir = \"logs/{}/{}\".format(now, f_name)\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    recon_error_s = 0\n",
    "    res_perplexity = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device, dtype=torch.float)\n",
    "        data = data.view(data.size(0), 1, -1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        vq_loss, data_recon, perplexity = model(data)\n",
    "        recon_error = F.mse_loss(data_recon, data) / data_variance        \n",
    "        loss = recon_error + vq_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        recon_error_s += recon_error.item()\n",
    "        res_perplexity += perplexity.item()\n",
    "        \n",
    "#         recon_error_s += recon_error.item()\n",
    "#         res_perplexity += perplexity.item()\n",
    "        \n",
    "#         # マルチGPU用\n",
    "#         loss = loss.mean()\n",
    "#         #loss.mean().backward()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         perplexity = perplexity.mean()\n",
    "#         recon_error_s += recon_error.item()\n",
    "#         res_perplexity += perplexity.item()\n",
    "    writer.add_scalar(\"train-loss\", recon_error_s/len(train_loader), epoch)\n",
    "    writer.add_scalar(\"train-perplexity\", res_perplexity/len(train_loader), epoch)\n",
    "    \n",
    "    recon_error_s = 0\n",
    "    res_perplexity = 0\n",
    "    #検証データで評価\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            data = data.to(device, dtype=torch.float)\n",
    "            data = data.view(data.size(0), 1, -1)\n",
    "            vq_loss, data_recon, perplexity = model(data)\n",
    "            recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "            recon_error_s += recon_error.item()\n",
    "            res_perplexity += perplexity.item()\n",
    "\n",
    "    train_res_recon_error.append(recon_error_s/len(valid_loader))\n",
    "    train_res_perplexity.append(res_perplexity/len(valid_loader))\n",
    "    writer.add_scalar(\"valid-loss\", recon_error_s/len(valid_loader), epoch)\n",
    "    writer.add_scalar(\"valid-perplexity\", res_perplexity/len(valid_loader), epoch)\n",
    "    if recon_error_s/len(train_loader) <= train_res_recon_error[good_id]:\n",
    "        torch.save(model.state_dict(), name)\n",
    "        good_id = epoch\n",
    "\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQ_VAE(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "model.load_state_dict(torch.load('pth/emg_vq_vae-256-8dim.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        data = data.view(data.size(0), 1, -1)\n",
    "        data = data.float()\n",
    "        data = data.to(device)\n",
    "        print(data.shape)\n",
    "        vq_loss, data_recon, perplexity = model(data)\n",
    "#         loss = criterion(net.reconstruction(outputs), data)\n",
    "#         valid_loss += loss.item()\n",
    "        break\n",
    "    \n",
    "axs = []\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 2, i+1)\n",
    "    axs.append(ax)\n",
    "reco = data_recon\n",
    "reco = reco.reshape(-1, 750)\n",
    "t = [i for i in range(200)]\n",
    "#t = [i for i in range(15000)]\n",
    "for i in range(5):\n",
    "    y1 = reco[i].to('cpu').detach().numpy().copy()\n",
    "    y2 = data[i].to('cpu').detach().numpy().copy()\n",
    "    y2 = y2.reshape(750)\n",
    "\n",
    "    axs[i*2].plot(t, y1[:200], color='blue', label='reconstruction')\n",
    "    axs[i*2+1].plot(t, y2[:200], color='red', label='original')\n",
    "#     axs[i*2].plot(t, y1, color='blue', label='reconstruction')\n",
    "#     axs[i*2+1].plot(t, y2, color='red', label='original')\n",
    "fig.tight_layout()\n",
    "#plt.savefig('1500.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 復元誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQ_VAE(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "model.load_state_dict(torch.load('pth/gaze_vq_vae-128-32dim.pth'))\n",
    "criterion = nn.MSELoss()\n",
    "model.eval()\n",
    "valid_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        data = data.view(data.size(0), 1, -1)\n",
    "        data = data.float()\n",
    "        data = data.to(device)\n",
    "        vq_loss, data_recon, perplexity = model(data)\n",
    "        loss = criterion(data_recon, data)\n",
    "        valid_loss += loss.item()\n",
    "valid_loss = valid_loss / len(valid_loader)\n",
    "print(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 食べ物のデータ処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQ_VAE(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "model.load_state_dict(torch.load('pth/gaze_vq_vae-256-8dim.pth'))\n",
    "batch_size = 30\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "# net2 = SparseAutoEncoder(1500, 1000).to(device)\n",
    "# net2.load_state_dict(torch.load('emg1500_1000.pth'))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "valid_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.view(data.size(0), 1, -1)\n",
    "        data = data.float()\n",
    "        data = data.to(device)\n",
    "        z = model._encoder(data)\n",
    "        print(z.shape)\n",
    "        z = model._pre_vq_conv(z)\n",
    "        print(z.shape)\n",
    "        loss, quantized, perplexity, _ = model._vq_vae(z)\n",
    "        break\n",
    "\n",
    "out = quantized.to('cpu').detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token_idを取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_id_from_vqvae(model, data):\n",
    "        bsz = data.shape[0]\n",
    "        z = model._pre_vq_conv( model._encoder(data) ) \n",
    "        loss, quantized, perplexity, vq_vae_encodings = model._vq_vae(z)\n",
    "        #token_ids = vq_vae_encodings.argmax(1).view(bsz,-1)\n",
    "        token_ids = vq_vae_encodings.argmax(2).view(bsz,-1)\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = get_token_id_from_vqvae(model, data)\n",
    "id = id.to('cpu').detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_all = []\n",
    "for i in range(30):\n",
    "    ls = []\n",
    "    for j in range(num_embeddings):\n",
    "        count = 0\n",
    "        for k in range(len(id[0])):\n",
    "            if(id[i][k] == j):\n",
    "                count += 1\n",
    "        ls.append(count)\n",
    "    ls_all.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"emg_VAE_npy/emg_vq_vae-256-8-testdim\", ls_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用されているコーパス数を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flag = [False] * num_embeddings\n",
    "for i in range(len(id)):\n",
    "    for j in range(len(id[i])):\n",
    "        use_flag[id[i][j]] = True\n",
    "\n",
    "use_flag.count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoderへの入力のPCA解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "pca = PCA()\n",
    "\n",
    "out = out.reshape(30, -1)\n",
    "\n",
    "# 中間層の標準化\n",
    "ss = preprocessing.StandardScaler()\n",
    "print(out.shape)\n",
    "encode = ss.fit_transform(out)\n",
    "\n",
    "label = [1, 2, 3, 4, 5, 6]\n",
    "labels = []\n",
    "for i in label:\n",
    "    labels.extend([i, i, i, i, i])\n",
    "\n",
    "pca.fit(encode)\n",
    "feature = pca.transform(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "label = ['cherry', 'meatball', 'watermelon', 'tomato', 'salted plum', 'wiener']\n",
    "# ax.scatter(feature[:5,0],feature[:5,1], c='red', marker='.', label='cherry')\n",
    "# ax.scatter(feature[5:10,0],feature[5:10,1], c='blue',marker='o', label='meatball')\n",
    "# ax.scatter(feature[10:15,0],feature[10:15,1], c='green',marker='^', label='watermelon')\n",
    "# ax.scatter(feature[15:20,0],feature[15:20,1], c='yellow',marker='s', label='tomato')\n",
    "# ax.scatter(feature[20:25,0],feature[20:25,1], c='m',marker='x', label='salted plum')\n",
    "# ax.scatter(feature[25:,0],feature[25:,1], c='k',marker='+', label='wiener')\n",
    "ax.scatter(feature[:5,0],feature[:5,1], c='red', marker='.', label='cherry')\n",
    "ax.scatter(feature[5:10,0],feature[5:10,1], c='blue',marker='o', label='meatball')\n",
    "ax.scatter(feature[10:15,0],feature[10:15,1], c='red',marker='^', label='watermelon')\n",
    "ax.scatter(feature[15:20,0],feature[15:20,1], c='blue',marker='s', label='tomato')\n",
    "ax.scatter(feature[20:25,0],feature[20:25,1], c='blue',marker='x', label='salted plum')\n",
    "ax.scatter(feature[25:,0],feature[25:,1], c='blue',marker='+', label='wiener')\n",
    "\n",
    "#ax.set_title('fourth scatter plot')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2', rotation=0)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "#ax.legend(loc='upper right')\n",
    "#lgd=ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=18)\n",
    "# name = \"pdf/emg_vq_vae-{}-{}.pdf\".format(num_embeddings, embedding_dim)\n",
    "# fig.savefig(name, bbox_inch='tight')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
